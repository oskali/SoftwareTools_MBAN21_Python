{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We'll take a tour of the methods for classification in sklearn. First let's load a toy dataset to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert it to a dataframe for better visuals\n",
    "df = pd.DataFrame(breast.data)\n",
    "df.columns = breast.feature_names\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now look at the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(breast.target_names)\n",
    "breast.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scikit learn models is basically the same as in Julia's ScikitLearn.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2, min_samples_leaf=140)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "cart = DecisionTreeClassifier(max_depth=2, min_samples_leaf=140)\n",
    "cart.fit(breast.data, breast.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helper function to plot the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Graphviz (tedious)\n",
    "\n",
    "## Windows\n",
    "\n",
    "1. Download graphviz from https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "2. Install it by running the .msi file\n",
    "3. Set the pat variable:\n",
    "    (a) Go to Control Panel > System and Security > System > Advanced System Settings >  Environment Variables > Path > Edit\n",
    "    (b) Add 'C:\\Program Files (x86)\\Graphviz2.38\\bin'\n",
    "4. Run `conda install graphviz`\n",
    "5. Run `conda install python-graphviz`\n",
    "\n",
    "## macOS and Linux\n",
    "\n",
    "1. Run `brew install graphviz` (install `brew` from https://docs.brew.sh/Installation if you don't have it)\n",
    "2. Run `conda install graphviz`\n",
    "3. Run `conda install python-graphviz`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import sklearn.tree\n",
    "def visualize_tree(sktree):\n",
    "    dot_data = sklearn.tree.export_graphviz(sktree, out_file=None, \n",
    "                                    filled=True, rounded=True,  \n",
    "                                    special_characters=False,\n",
    "                                    feature_names=df.columns)\n",
    "    return graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"360pt\" height=\"269pt\"\r\n",
       " viewBox=\"0.00 0.00 359.50 269.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 265)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-265 355.5,-265 355.5,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#afd7f4\" stroke=\"black\" d=\"M276,-261C276,-261 139,-261 139,-261 133,-261 127,-255 127,-249 127,-249 127,-205 127,-205 127,-199 133,-193 139,-193 139,-193 276,-193 276,-193 282,-193 288,-199 288,-205 288,-205 288,-249 288,-249 288,-255 282,-261 276,-261\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"207.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">worst radius &lt;= 16.795</text>\r\n",
       "<text text-anchor=\"middle\" x=\"207.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.468</text>\r\n",
       "<text text-anchor=\"middle\" x=\"207.5\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 569</text>\r\n",
       "<text text-anchor=\"middle\" x=\"207.5\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [212, 357]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#4ca6e7\" stroke=\"black\" d=\"M199,-157C199,-157 50,-157 50,-157 44,-157 38,-151 38,-145 38,-145 38,-101 38,-101 38,-95 44,-89 50,-89 50,-89 199,-89 199,-89 205,-89 211,-95 211,-101 211,-101 211,-145 211,-145 211,-151 205,-157 199,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"124.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">worst concavity &lt;= 0.189</text>\r\n",
       "<text text-anchor=\"middle\" x=\"124.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.159</text>\r\n",
       "<text text-anchor=\"middle\" x=\"124.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 379</text>\r\n",
       "<text text-anchor=\"middle\" x=\"124.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [33, 346]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M180.552,-192.884C173.377,-184.065 165.536,-174.43 158.054,-165.235\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.623,-162.847 151.597,-157.299 155.193,-167.265 160.623,-162.847\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"149.042\" y=\"-178.469\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#e78945\" stroke=\"black\" d=\"M339.5,-149.5C339.5,-149.5 241.5,-149.5 241.5,-149.5 235.5,-149.5 229.5,-143.5 229.5,-137.5 229.5,-137.5 229.5,-108.5 229.5,-108.5 229.5,-102.5 235.5,-96.5 241.5,-96.5 241.5,-96.5 339.5,-96.5 339.5,-96.5 345.5,-96.5 351.5,-102.5 351.5,-108.5 351.5,-108.5 351.5,-137.5 351.5,-137.5 351.5,-143.5 345.5,-149.5 339.5,-149.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"290.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.109</text>\r\n",
       "<text text-anchor=\"middle\" x=\"290.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 190</text>\r\n",
       "<text text-anchor=\"middle\" x=\"290.5\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [179, 11]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>0&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M234.448,-192.884C243.665,-181.556 253.98,-168.88 263.223,-157.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"266.137,-159.486 269.734,-149.52 260.707,-155.067 266.137,-159.486\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"272.288\" y=\"-170.689\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#3b9ee5\" stroke=\"black\" d=\"M101,-53C101,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 101,-0 101,-0 107,-0 113,-6 113,-12 113,-12 113,-41 113,-41 113,-47 107,-53 101,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.017</text>\r\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 239</text>\r\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2, 237]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.717,-88.9485C94.3317,-80.0749 87.4165,-70.4648 80.9935,-61.5388\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"83.7013,-59.3094 75.0195,-53.2367 78.0194,-63.398 83.7013,-59.3094\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#71b9ec\" stroke=\"black\" d=\"M241.5,-53C241.5,-53 143.5,-53 143.5,-53 137.5,-53 131.5,-47 131.5,-41 131.5,-41 131.5,-12 131.5,-12 131.5,-6 137.5,-0 143.5,-0 143.5,-0 241.5,-0 241.5,-0 247.5,-0 253.5,-6 253.5,-12 253.5,-12 253.5,-41 253.5,-41 253.5,-47 247.5,-53 241.5,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"192.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.345</text>\r\n",
       "<text text-anchor=\"middle\" x=\"192.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 140</text>\r\n",
       "<text text-anchor=\"middle\" x=\"192.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 109]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M148.283,-88.9485C154.668,-80.0749 161.584,-70.4648 168.007,-61.5388\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.981,-63.398 173.98,-53.2367 165.299,-59.3094 170.981,-63.398\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x228e2399c50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_tree(cart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can get the label predictions with the `.predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = cart.predict(breast.data)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly the predicted probabilities with `.predict_proba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94210526, 0.05789474],\n",
       "       [0.94210526, 0.05789474],\n",
       "       [0.94210526, 0.05789474],\n",
       "       ...,\n",
       "       [0.94210526, 0.05789474],\n",
       "       [0.94210526, 0.05789474],\n",
       "       [0.0083682 , 0.9916318 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = cart.predict_proba(breast.data)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in Julia, the probabilities are returned for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the second column of the probs by slicing, just like how we did it in Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05789474, 0.05789474, 0.05789474, 0.77857143, 0.05789474,\n",
       "       0.77857143, 0.05789474, 0.05789474, 0.77857143, 0.77857143,\n",
       "       0.05789474, 0.05789474, 0.05789474, 0.05789474, 0.77857143,\n",
       "       0.05789474, 0.05789474, 0.05789474, 0.05789474, 0.77857143,\n",
       "       0.77857143, 0.9916318 , 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.05789474, 0.05789474, 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.05789474, 0.05789474, 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.05789474, 0.77857143, 0.9916318 , 0.9916318 , 0.77857143,\n",
       "       0.77857143, 0.77857143, 0.05789474, 0.05789474, 0.77857143,\n",
       "       0.05789474, 0.9916318 , 0.77857143, 0.77857143, 0.77857143,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.05789474, 0.05789474,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.9916318 , 0.05789474,\n",
       "       0.05789474, 0.9916318 , 0.9916318 , 0.77857143, 0.9916318 ,\n",
       "       0.05789474, 0.9916318 , 0.05789474, 0.77857143, 0.9916318 ,\n",
       "       0.05789474, 0.9916318 , 0.05789474, 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.05789474, 0.05789474, 0.77857143,\n",
       "       0.05789474, 0.77857143, 0.05789474, 0.77857143, 0.77857143,\n",
       "       0.9916318 , 0.77857143, 0.9916318 , 0.9916318 , 0.05789474,\n",
       "       0.05789474, 0.9916318 , 0.9916318 , 0.77857143, 0.77857143,\n",
       "       0.05789474, 0.9916318 , 0.9916318 , 0.77857143, 0.9916318 ,\n",
       "       0.77857143, 0.77857143, 0.77857143, 0.05789474, 0.77857143,\n",
       "       0.9916318 , 0.77857143, 0.77857143, 0.9916318 , 0.77857143,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.77857143, 0.05789474, 0.05789474, 0.77857143, 0.77857143,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.77857143, 0.05789474,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.77857143, 0.9916318 , 0.9916318 , 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.05789474, 0.9916318 , 0.77857143, 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.77857143, 0.77857143, 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.77857143, 0.9916318 , 0.77857143,\n",
       "       0.77857143, 0.05789474, 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.77857143, 0.05789474, 0.05789474, 0.9916318 , 0.05789474,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.05789474, 0.05789474, 0.05789474, 0.9916318 , 0.05789474,\n",
       "       0.9916318 , 0.05789474, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.77857143, 0.9916318 , 0.9916318 , 0.77857143, 0.77857143,\n",
       "       0.9916318 , 0.77857143, 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.05789474, 0.77857143,\n",
       "       0.05789474, 0.9916318 , 0.05789474, 0.77857143, 0.05789474,\n",
       "       0.05789474, 0.9916318 , 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.77857143, 0.77857143, 0.77857143, 0.05789474, 0.05789474,\n",
       "       0.9916318 , 0.77857143, 0.9916318 , 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.77857143, 0.77857143, 0.77857143,\n",
       "       0.05789474, 0.9916318 , 0.9916318 , 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.77857143, 0.05789474,\n",
       "       0.9916318 , 0.9916318 , 0.77857143, 0.9916318 , 0.05789474,\n",
       "       0.9916318 , 0.77857143, 0.77857143, 0.9916318 , 0.9916318 ,\n",
       "       0.05789474, 0.9916318 , 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.77857143, 0.05789474, 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.05789474, 0.05789474, 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.05789474, 0.77857143, 0.9916318 , 0.77857143, 0.77857143,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.9916318 , 0.05789474,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.05789474, 0.9916318 , 0.05789474, 0.05789474, 0.77857143,\n",
       "       0.9916318 , 0.77857143, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.77857143, 0.9916318 , 0.77857143, 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.05789474, 0.77857143, 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.77857143, 0.9916318 ,\n",
       "       0.77857143, 0.05789474, 0.9916318 , 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.05789474, 0.05789474,\n",
       "       0.05789474, 0.77857143, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.05789474, 0.9916318 , 0.05789474, 0.9916318 , 0.05789474,\n",
       "       0.77857143, 0.77857143, 0.77857143, 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.05789474, 0.9916318 ,\n",
       "       0.77857143, 0.77857143, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.05789474, 0.9916318 ,\n",
       "       0.05789474, 0.05789474, 0.77857143, 0.05789474, 0.05789474,\n",
       "       0.05789474, 0.9916318 , 0.05789474, 0.05789474, 0.9916318 ,\n",
       "       0.05789474, 0.77857143, 0.9916318 , 0.77857143, 0.77857143,\n",
       "       0.77857143, 0.9916318 , 0.77857143, 0.77857143, 0.77857143,\n",
       "       0.77857143, 0.77857143, 0.9916318 , 0.77857143, 0.05789474,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.77857143, 0.77857143, 0.9916318 ,\n",
       "       0.05789474, 0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.05789474, 0.9916318 , 0.05789474, 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.77857143, 0.05789474,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.77857143, 0.77857143, 0.77857143, 0.77857143, 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.77857143, 0.9916318 , 0.9916318 ,\n",
       "       0.77857143, 0.77857143, 0.05789474, 0.05789474, 0.9916318 ,\n",
       "       0.05789474, 0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.77857143, 0.05789474, 0.9916318 , 0.9916318 , 0.05789474,\n",
       "       0.9916318 , 0.05789474, 0.77857143, 0.77857143, 0.05789474,\n",
       "       0.77857143, 0.05789474, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.05789474, 0.05789474, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.77857143, 0.77857143, 0.9916318 , 0.05789474, 0.77857143,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.9916318 , 0.77857143,\n",
       "       0.77857143, 0.77857143, 0.9916318 , 0.77857143, 0.05789474,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 , 0.05789474,\n",
       "       0.77857143, 0.77857143, 0.05789474, 0.9916318 , 0.05789474,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.77857143, 0.05789474, 0.05789474,\n",
       "       0.9916318 , 0.77857143, 0.77857143, 0.05789474, 0.77857143,\n",
       "       0.77857143, 0.77857143, 0.9916318 , 0.05789474, 0.05789474,\n",
       "       0.77857143, 0.9916318 , 0.77857143, 0.77857143, 0.05789474,\n",
       "       0.9916318 , 0.05789474, 0.05789474, 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.05789474, 0.9916318 , 0.77857143, 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.77857143, 0.9916318 , 0.05789474, 0.77857143,\n",
       "       0.05789474, 0.77857143, 0.9916318 , 0.9916318 , 0.77857143,\n",
       "       0.9916318 , 0.77857143, 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 ,\n",
       "       0.9916318 , 0.9916318 , 0.9916318 , 0.9916318 , 0.77857143,\n",
       "       0.77857143, 0.9916318 , 0.9916318 , 0.77857143, 0.77857143,\n",
       "       0.9916318 , 0.9916318 , 0.05789474, 0.05789474, 0.05789474,\n",
       "       0.05789474, 0.05789474, 0.05789474, 0.9916318 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = cart.predict_proba(breast.data)[:,1]\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we can use functions from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538607895988584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(breast.target, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226713532513181"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(breast.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[179,  33],\n",
       "       [ 11, 346]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(breast.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests and Boosting\n",
    "\n",
    "We use random forests and boosting in the same way as CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest.fit(breast.data, breast.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[212,   0],\n",
       "       [  0, 357]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = forest.predict(breast.data)\n",
    "probs = forest.predict_proba(breast.data)[:,1]\n",
    "print(roc_auc_score(breast.target, probs))\n",
    "print(accuracy_score(breast.target, labels))\n",
    "confusion_matrix(breast.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "boost = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)\n",
    "boost.fit(breast.data, breast.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[212,   0],\n",
       "       [  0, 357]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = boost.predict(breast.data)\n",
    "probs = boost.predict_proba(breast.data)[:,1]\n",
    "print(roc_auc_score(breast.target, probs))\n",
    "print(accuracy_score(breast.target, labels))\n",
    "confusion_matrix(breast.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "boost2 = XGBClassifier()\n",
    "boost2.fit(breast.data, breast.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[212,   0],\n",
       "       [  0, 357]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = boost2.predict(breast.data)\n",
    "probs = boost2.predict_proba(breast.data)[:,1]\n",
    "print(roc_auc_score(breast.target, probs))\n",
    "print(accuracy_score(breast.target, labels))\n",
    "confusion_matrix(breast.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(breast.data, breast.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9890201363564294\n",
      "0.9525483304042179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[190,  22],\n",
       "       [  5, 352]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = mlp.predict(breast.data)\n",
    "probs = mlp.predict_proba(breast.data)[:,1]\n",
    "print(roc_auc_score(breast.target, probs))\n",
    "print(accuracy_score(breast.target, labels))\n",
    "confusion_matrix(breast.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 232.5632 - accuracy: 0.3582\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 168.5181 - accuracy: 0.3582\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 111.4261 - accuracy: 0.3582\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 59.1086 - accuracy: 0.3582\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 11.6282 - accuracy: 0.5385\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.7738 - accuracy: 0.6945\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.3742 - accuracy: 0.7011\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.2461 - accuracy: 0.7165\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.1334 - accuracy: 0.7297\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.1214 - accuracy: 0.7297\n",
      "8/8 [==============================] - 0s 995us/step - loss: 2.8094 - accuracy: 0.6754\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 146.7898 - accuracy: 0.6374\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 952us/step - loss: 88.7009 - accuracy: 0.6374\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 38.3368 - accuracy: 0.6374\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 5.7533 - accuracy: 0.3626\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.1352 - accuracy: 0.2703\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1898 - accuracy: 0.3956\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5838 - accuracy: 0.7187\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4672 - accuracy: 0.7758\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3619 - accuracy: 0.8681\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.8967\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8333\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 928us/step - loss: 200.6983 - accuracy: 0.3736\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 929us/step - loss: 129.0806 - accuracy: 0.3736\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 69.3608 - accuracy: 0.3736\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 19.2394 - accuracy: 0.4637\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 981us/step - loss: 2.4149 - accuracy: 0.8703\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 964us/step - loss: 1.6610 - accuracy: 0.8264\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 976us/step - loss: 1.6249 - accuracy: 0.8418\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.6103 - accuracy: 0.8396\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.5151 - accuracy: 0.8462\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.4719 - accuracy: 0.8527\n",
      "8/8 [==============================] - 0s 997us/step - loss: 2.7245 - accuracy: 0.8772\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 14.3345 - accuracy: 0.4967\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 879us/step - loss: 5.5267 - accuracy: 0.6110\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 991us/step - loss: 3.5753 - accuracy: 0.6989\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 999us/step - loss: 2.4254 - accuracy: 0.7802\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.7139 - accuracy: 0.8088\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.5973 - accuracy: 0.8505\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2708 - accuracy: 0.8527\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1576 - accuracy: 0.8681\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1389 - accuracy: 0.8747\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.9703 - accuracy: 0.8681\n",
      "8/8 [==============================] - 0s 867us/step - loss: 1.4932 - accuracy: 0.8070\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 207.4908 - accuracy: 0.3838\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 90.8189 - accuracy: 0.3838\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 11.5153 - accuracy: 0.5263\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.6981 - accuracy: 0.6031\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1285 - accuracy: 0.7215\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.0299 - accuracy: 0.7149\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.9996 - accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8036 - accuracy: 0.8004\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8759 - accuracy: 0.7851\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7534 - accuracy: 0.7939\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6084 - accuracy: 0.8319\n",
      "Baseline: 80.50% (6.86%)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "X = breast.data\n",
    "Y = breast.target\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(Y)\n",
    " \n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=30, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=16, verbose=1)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We can also access logistic regression from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(solver='liblinear')\n",
    "logit.fit(breast.data, breast.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9945298874266688\n",
      "0.9578207381370826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[197,  15],\n",
       "       [  9, 348]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = logit.predict(breast.data)\n",
    "probs = logit.predict_proba(breast.data)[:,1]\n",
    "print(roc_auc_score(breast.target, probs))\n",
    "print(accuracy_score(breast.target, labels))\n",
    "confusion_matrix(breast.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn implementation has options for regularization in logistic regression. You can choose between L1 and L2 regularization:\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/math/6a0bcf21baaeb0c2b879ab74fe333c0aab0d6ae6.png)\n",
    "\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/math/760c999ccbc78b72d2a91186ba55ce37f0d2cf37.png)\n",
    "\n",
    "Note that this regularization is adhoc and **not equivalent to robustness**. For a robust logistic regression, follow the approach from 15.680.\n",
    "\n",
    "You control the regularization with the `penalty` and `C` hyperparameters. We can see that our model above used L2 regularization with $C=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try out unregularized logistic regression as well as L1 regularization. Which of the three options seems best? What if you try changing $C$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Now let's take a look at regression in sklearn. Again we can start by loading up a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(boston.data)\n",
    "df.columns = boston.feature_names\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "\n",
    "We use regression trees in the same way as classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"506pt\" height=\"269pt\"\r\n",
       " viewBox=\"0.00 0.00 505.50 269.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 265)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-265 501.5,-265 501.5,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#f8dfcd\" stroke=\"black\" d=\"M292,-261C292,-261 205,-261 205,-261 199,-261 193,-255 193,-249 193,-249 193,-205 193,-205 193,-199 199,-193 205,-193 205,-193 292,-193 292,-193 298,-193 304,-199 304,-205 304,-205 304,-249 304,-249 304,-255 298,-261 292,-261\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"248.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RM &lt;= 6.941</text>\r\n",
       "<text text-anchor=\"middle\" x=\"248.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 84.42</text>\r\n",
       "<text text-anchor=\"middle\" x=\"248.5\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 506</text>\r\n",
       "<text text-anchor=\"middle\" x=\"248.5\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 22.533</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#fbeade\" stroke=\"black\" d=\"M229,-157C229,-157 138,-157 138,-157 132,-157 126,-151 126,-145 126,-145 126,-101 126,-101 126,-95 132,-89 138,-89 138,-89 229,-89 229,-89 235,-89 241,-95 241,-101 241,-101 241,-145 241,-145 241,-151 235,-157 229,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"183.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">LSTAT &lt;= 14.4</text>\r\n",
       "<text text-anchor=\"middle\" x=\"183.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 40.273</text>\r\n",
       "<text text-anchor=\"middle\" x=\"183.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 430</text>\r\n",
       "<text text-anchor=\"middle\" x=\"183.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 19.934</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M227.396,-192.884C221.89,-184.243 215.885,-174.819 210.133,-165.793\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"213.046,-163.852 204.72,-157.299 207.143,-167.614 213.046,-163.852\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"199.312\" y=\"-178.007\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#eca26d\" stroke=\"black\" d=\"M357.5,-157C357.5,-157 271.5,-157 271.5,-157 265.5,-157 259.5,-151 259.5,-145 259.5,-145 259.5,-101 259.5,-101 259.5,-95 265.5,-89 271.5,-89 271.5,-89 357.5,-89 357.5,-89 363.5,-89 369.5,-95 369.5,-101 369.5,-101 369.5,-145 369.5,-145 369.5,-151 363.5,-157 357.5,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RM &lt;= 7.437</text>\r\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 79.729</text>\r\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 76</text>\r\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 37.238</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>0&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269.928,-192.884C275.519,-184.243 281.617,-174.819 287.458,-165.793\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.459,-167.596 292.953,-157.299 284.582,-163.794 290.459,-167.596\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"298.192\" y=\"-178.044\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#f8dcc8\" stroke=\"black\" d=\"M99,-53C99,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 99,-0 99,-0 105,-0 111,-6 111,-12 111,-12 111,-41 111,-41 111,-47 105,-53 99,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 26.009</text>\r\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 255</text>\r\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 23.35</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M138.732,-88.9485C125.721,-79.3431 111.542,-68.8747 98.634,-59.345\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.484,-56.3605 90.3603,-53.2367 96.3265,-61.992 100.484,-56.3605\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M228,-53C228,-53 141,-53 141,-53 135,-53 129,-47 129,-41 129,-41 129,-12 129,-12 129,-6 135,-0 141,-0 141,-0 228,-0 228,-0 234,-0 240,-6 240,-12 240,-12 240,-41 240,-41 240,-47 234,-53 228,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 19.276</text>\r\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 175</text>\r\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 14.956</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M183.85,-88.9485C183.937,-80.7153 184.031,-71.848 184.119,-63.4814\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"187.622,-63.2732 184.228,-53.2367 180.622,-63.1991 187.622,-63.2732\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#f0b78e\" stroke=\"black\" d=\"M357.5,-53C357.5,-53 271.5,-53 271.5,-53 265.5,-53 259.5,-47 259.5,-41 259.5,-41 259.5,-12 259.5,-12 259.5,-6 265.5,-0 271.5,-0 271.5,-0 357.5,-0 357.5,-0 363.5,-0 369.5,-6 369.5,-12 369.5,-12 369.5,-41 369.5,-41 369.5,-47 363.5,-53 357.5,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 41.296</text>\r\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\r\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 32.113</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M314.5,-88.9485C314.5,-80.7153 314.5,-71.848 314.5,-63.4814\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"318,-63.2367 314.5,-53.2367 311,-63.2367 318,-63.2367\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M485.5,-53C485.5,-53 399.5,-53 399.5,-53 393.5,-53 387.5,-47 387.5,-41 387.5,-41 387.5,-12 387.5,-12 387.5,-6 393.5,-0 399.5,-0 399.5,-0 485.5,-0 485.5,-0 491.5,-0 497.5,-6 497.5,-12 497.5,-12 497.5,-41 497.5,-41 497.5,-47 491.5,-53 485.5,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"442.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 36.628</text>\r\n",
       "<text text-anchor=\"middle\" x=\"442.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\r\n",
       "<text text-anchor=\"middle\" x=\"442.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 45.097</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>4&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M359.268,-88.9485C372.279,-79.3431 386.458,-68.8747 399.366,-59.345\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"401.673,-61.992 407.64,-53.2367 397.516,-56.3605 401.673,-61.992\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x228e388f278>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "cart = DecisionTreeRegressor(max_depth=2, min_samples_leaf=5)\n",
    "cart.fit(boston.data, boston.target)\n",
    "visualize_tree(cart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for classification, we get the predicted labels out with the `.predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.34980392, 23.34980392, 32.11304348, 32.11304348, 32.11304348,\n",
       "       23.34980392, 23.34980392, 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 23.34980392, 14.956     , 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 14.956     , 23.34980392, 23.34980392,\n",
       "       14.956     , 23.34980392, 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 23.34980392, 23.34980392,\n",
       "       14.956     , 23.34980392, 14.956     , 14.956     , 14.956     ,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       32.11304348, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 14.956     , 14.956     , 14.956     ,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 14.956     ,\n",
       "       32.11304348, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 14.956     , 23.34980392, 23.34980392, 32.11304348,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 32.11304348, 32.11304348,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 45.09666667, 45.09666667, 32.11304348,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       14.956     , 14.956     , 23.34980392, 23.34980392, 14.956     ,\n",
       "       23.34980392, 23.34980392, 14.956     , 14.956     , 23.34980392,\n",
       "       14.956     , 23.34980392, 23.34980392, 14.956     , 23.34980392,\n",
       "       23.34980392, 23.34980392, 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       23.34980392, 23.34980392, 23.34980392, 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       23.34980392, 23.34980392, 23.34980392, 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 32.11304348, 23.34980392, 23.34980392,\n",
       "       23.34980392, 45.09666667, 45.09666667, 45.09666667, 23.34980392,\n",
       "       23.34980392, 45.09666667, 23.34980392, 23.34980392, 23.34980392,\n",
       "       14.956     , 23.34980392, 14.956     , 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 32.11304348,\n",
       "       45.09666667, 23.34980392, 32.11304348, 23.34980392, 23.34980392,\n",
       "       23.34980392, 45.09666667, 23.34980392, 23.34980392, 32.11304348,\n",
       "       32.11304348, 23.34980392, 32.11304348, 23.34980392, 23.34980392,\n",
       "       45.09666667, 32.11304348, 32.11304348, 32.11304348, 32.11304348,\n",
       "       32.11304348, 23.34980392, 45.09666667, 45.09666667, 45.09666667,\n",
       "       23.34980392, 23.34980392, 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 23.34980392, 14.956     ,\n",
       "       23.34980392, 23.34980392, 23.34980392, 14.956     , 23.34980392,\n",
       "       32.11304348, 14.956     , 23.34980392, 23.34980392, 45.09666667,\n",
       "       45.09666667, 45.09666667, 32.11304348, 45.09666667, 23.34980392,\n",
       "       23.34980392, 32.11304348, 45.09666667, 45.09666667, 23.34980392,\n",
       "       23.34980392, 23.34980392, 32.11304348, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       14.956     , 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 32.11304348, 45.09666667, 23.34980392,\n",
       "       23.34980392, 45.09666667, 45.09666667, 32.11304348, 23.34980392,\n",
       "       32.11304348, 45.09666667, 45.09666667, 32.11304348, 32.11304348,\n",
       "       23.34980392, 32.11304348, 45.09666667, 45.09666667, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 45.09666667, 23.34980392,\n",
       "       23.34980392, 32.11304348, 23.34980392, 23.34980392, 23.34980392,\n",
       "       45.09666667, 32.11304348, 45.09666667, 45.09666667, 32.11304348,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 32.11304348, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 14.956     , 23.34980392, 32.11304348,\n",
       "       23.34980392, 23.34980392, 23.34980392, 32.11304348, 32.11304348,\n",
       "       23.34980392, 32.11304348, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 14.956     , 14.956     , 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 32.11304348, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 14.956     , 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 23.34980392, 23.34980392, 14.956     , 45.09666667,\n",
       "       23.34980392, 23.34980392, 23.34980392, 23.34980392, 23.34980392,\n",
       "       32.11304348, 23.34980392, 23.34980392, 14.956     , 14.956     ,\n",
       "       32.11304348, 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       32.11304348, 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 23.34980392, 14.956     , 14.956     ,\n",
       "       23.34980392, 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 23.34980392, 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 23.34980392, 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 32.11304348, 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 23.34980392, 23.34980392, 23.34980392,\n",
       "       23.34980392, 14.956     , 14.956     , 14.956     , 14.956     ,\n",
       "       14.956     , 23.34980392, 23.34980392, 32.11304348, 14.956     ,\n",
       "       14.956     , 14.956     , 14.956     , 14.956     , 23.34980392,\n",
       "       23.34980392, 23.34980392, 32.11304348, 23.34980392, 23.34980392,\n",
       "       23.34980392, 14.956     , 23.34980392, 14.956     , 14.956     ,\n",
       "       14.956     , 14.956     , 23.34980392, 23.34980392, 23.34980392,\n",
       "       14.956     , 14.956     , 23.34980392, 23.34980392, 14.956     ,\n",
       "       23.34980392, 23.34980392, 23.34980392, 32.11304348, 23.34980392,\n",
       "       23.34980392])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = cart.predict(boston.data)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are functions provided by `sklearn.metrics` to evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5736909785051654\n",
      "25.699467452126065\n",
      "0.6955744779730269\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(boston.target, preds))\n",
    "print(mean_squared_error(boston.target, preds))\n",
    "print(r2_score(boston.target, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests and Boosting\n",
    "\n",
    "Random forests and boosting for regression work the same as in classification, except we use the `Regressor` version rather than `Classifier`.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Test and compare the (in-sample) performance of random forests and boosting on the Boston data with some sensible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(max_iter=1000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(max_iter=1000)\n",
    "mlp.fit(boston.data, boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.445522851106868\n",
      "23.83769995006188\n",
      "0.7176282246027788\n"
     ]
    }
   ],
   "source": [
    "preds = mlp.predict(boston.data)\n",
    "print(mean_absolute_error(boston.target, preds))\n",
    "print(mean_squared_error(boston.target, preds))\n",
    "print(r2_score(boston.target, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 366.3648\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 179.6168\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 149.6941\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 130.6266\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 114.1144\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 98.6075\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 88.2580\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 0s 959us/step - loss: 80.6100\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 75.5286\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 71.8456\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 53.1635\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 513.3400\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 267.2325\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 0s 999us/step - loss: 135.1917\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 116.1713\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 105.8186\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 96.5651\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 88.0007\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 81.1619\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 74.5669\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 69.9957\n",
      "7/7 [==============================] - 0s 999us/step - loss: 98.6647\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 665.9130\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 274.5114\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 0s 954us/step - loss: 159.9927\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 141.4306\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 128.8526\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 115.0151\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 103.9226\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 92.4595\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 85.7887\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 80.5021\n",
      "7/7 [==============================] - 0s 860us/step - loss: 48.6653\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 374.3582\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 123.8381\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 93.6649\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 80.4202\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 72.3477\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 67.4980\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 64.0814\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 62.6879\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 62.6356\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 60.3531\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 70.6915\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 269.4930\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 0s 997us/step - loss: 113.5610\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 93.6239\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 0s 997us/step - loss: 83.3438\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 78.0547\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 74.1171\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 70.9707\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 68.5240\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 66.5780\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 0s 960us/step - loss: 64.7309\n",
      "7/7 [==============================] - 0s 998us/step - loss: 70.5091\n",
      "Mean Squared Error: 68.34 (17.59)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "X = boston.data\n",
    "Y = boston.target\n",
    " \n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    " \n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=16, verbose=1)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Mean Squared Error: %.2f (%.2f)\" % (abs(results.mean()), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a large collection of linear regression models in sklearn. Let's start with a simple ordinary linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.270862810900317\n",
      "21.894831181729206\n",
      "0.7406426641094094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear = LinearRegression()\n",
    "linear.fit(boston.data, boston.target)\n",
    "preds = linear.predict(boston.data)\n",
    "print(mean_absolute_error(boston.target, preds))\n",
    "print(mean_squared_error(boston.target, preds))\n",
    "print(r2_score(boston.target, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the betas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.08011358e-01,  4.64204584e-02,  2.05586264e-02,  2.68673382e+00,\n",
       "       -1.77666112e+01,  3.80986521e+00,  6.92224640e-04, -1.47556685e+00,\n",
       "        3.06049479e-01, -1.23345939e-02, -9.52747232e-01,  9.31168327e-03,\n",
       "       -5.24758378e-01])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regularized models as well. Here is ridge regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.315169248123664\n",
      "22.660363555639318\n",
      "0.7315744764907257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.10143535,  0.0495791 , -0.0429624 ,  1.95202082, -2.37161896,\n",
       "        3.70227207, -0.01070735, -1.24880821,  0.2795956 , -0.01399313,\n",
       "       -0.79794498,  0.01003684, -0.55936642])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=10)\n",
    "ridge.fit(boston.data, boston.target)\n",
    "preds = ridge.predict(boston.data)\n",
    "print(mean_absolute_error(boston.target, preds))\n",
    "print(mean_squared_error(boston.target, preds))\n",
    "print(r2_score(boston.target, preds))\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6117102456478434\n",
      "26.79609915726647\n",
      "0.6825842212709925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.06343729,  0.04916467, -0.        ,  0.        , -0.        ,\n",
       "        0.9498107 ,  0.02090951, -0.66879   ,  0.26420643, -0.01521159,\n",
       "       -0.72296636,  0.00824703, -0.76111454])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1)\n",
    "lasso.fit(boston.data, boston.target)\n",
    "preds = lasso.predict(boston.data)\n",
    "print(mean_absolute_error(boston.target, preds))\n",
    "print(mean_squared_error(boston.target, preds))\n",
    "print(r2_score(boston.target, preds))\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other linear regression models available. See the [linear model documentation](http://scikit-learn.org/stable/modules/linear_model.html) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The elastic net is another linear regression method that combines ridge and lasso regularization. Try running it on this dataset, referring to the documentation as needed to learn how to use it and control the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
